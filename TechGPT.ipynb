{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TechGPT\n",
    "\n",
    "05/06/2025\n",
    "UNC Charlotte\n",
    "ITCS 5010 : Design and Development of Generative AI Applications\n",
    "Group 2: Cameron Detig, Zaid Jebril, Sri Girija Naga Anuhya Samudrala, Derek Smith\n",
    "\n",
    "- Reads a database of prior user queries and llm responses.\n",
    "- Uses queries to generate a new response using hidden chain of thought. \n",
    "- Evaluates the new response using ragas metrics.\n",
    "\n",
    "Steps to use:\n",
    "\n",
    "Create virtual environment and install everything in the requirements.txt\n",
    "\n",
    "Create a .env file and add your DATABRICKS_TOKEN and DATABRICKS_HOST API keys.\n",
    "\n",
    "In this directory add a .index vector database file, a .parquet metadata file, and a .csv file of test queries.\n",
    "Assign their file names to the variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_file_path = \"my_faiss.index\"\n",
    "metadata_file_path = \"metadata.parquet\"\n",
    "test_queries_csv_path = 'Feedback_PHD_cleaned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index loaded successfully!\n",
      "Metadata loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from databricks_langchain import ChatDatabricks, DatabricksEmbeddings\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy\n",
    "from ragas.evaluation import evaluate\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import ragas\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['DATABRICKS_TOKEN'] = os.getenv('DATABRICKS_TOKEN')\n",
    "os.environ['DATABRICKS_HOST'] = os.getenv('DATABRICKS_HOST')\n",
    "\n",
    "# Define llms\n",
    "chat_model = ChatDatabricks(\n",
    "    endpoint=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "\n",
    "find_exact_keyword = ChatDatabricks(\n",
    "    endpoint=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "\n",
    "# Define embedding model\n",
    "embedding_model = DatabricksEmbeddings(\n",
    "    endpoint = \"ada-002\"\n",
    ")\n",
    "\n",
    "\n",
    "# Load the FAISS index\n",
    "faiss_index = faiss.read_index(index_file_path)\n",
    "print(\"FAISS index loaded successfully!\")\n",
    "\n",
    "# Load metadata from the Parquet file\n",
    "metadata_df = pd.read_parquet(metadata_file_path)\n",
    "print(\"Metadata loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to be used by the callSystem function to run the response generation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(embedding_model, query):\n",
    "    '''Embeds the query using the specified embedding model.'''\n",
    "    embedding = embedding_model.embed_query(query)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def retrieve_chunks(index, query, k=5):\n",
    "    '''Retrieves the top k chunks from the FAISS index based on the query.'''\n",
    "    query_vector = get_embeddings(embedding_model, query)\n",
    "    query_vector = np.array(query_vector).astype(\"float32\").reshape(1,-1)\n",
    "\n",
    "    distance, indices = index.search(query_vector, k)\n",
    "\n",
    "    retrieved_chunks = metadata_df.iloc[indices[0]].reset_index(drop=True)\n",
    "\n",
    "    return retrieved_chunks\n",
    "\n",
    "\n",
    "def rewrite_query(user_query):\n",
    "    '''Rewrite the user's query to be more specific and concise'''\n",
    "\n",
    "    rewrite_query_prompt = \"\"\"\n",
    "    You are an expert in understanding and rephrasing user service queries based on chat history. Your goal is to break down the user's intent and context into a concise, reformulated question. \n",
    "\n",
    "    Internal Thought Process:\n",
    "\n",
    "    1. Analyze the user's current query {input}: Carefully examine the user's latest input to understand their core need and any specific details they have provided.\n",
    "    2. Consider the chat history: Review the provided {chat_history} to identify any relevant prior interactions. Play close attention to the flow of the conversation and the assitant's previous responses. If there is no chat history or if the chat history is not related to the user's current query, consider the query as a standalone query. \n",
    "    3. Identify the user's underlying intent: Based on current query and relevant history, determine what the user is really trying to achieve or understand. \n",
    "    4. Generate a context-rich question: Formulate a new, standalone question that accurately reflect's the user's intent, incorporating relevant context from the chat history. Ensure that this generated question is not a direct follow-up or answer to the current user query. \n",
    "    5. Refine the question: Ensure that the new question is clear, concise, relevant and in a proper question format. Avoid any repetition or ambiguity. \n",
    "\n",
    "    Output: Only the reformulated user question should be provided below\n",
    "\n",
    "    Based on the chat history: {chat_history}\n",
    "\n",
    "    What is the user's core question, taking into account the user's previous conversation?\n",
    "    \"\"\"\n",
    "\n",
    "    rewrite = PromptTemplate(\n",
    "                        template = rewrite_query_prompt,\n",
    "                        input_variables=[\"chat_history\", \"input\"]\n",
    "    )\n",
    "\n",
    "    rewritten_question_chain = rewrite | chat_model\n",
    "    rewritten_question = rewritten_question_chain.invoke({\"chat_history\": \"\", \"input\":user_query}).content\n",
    "\n",
    "    return rewritten_question\n",
    "\n",
    "\n",
    "def get_keywords_for_exact_search(user_query):\n",
    "    '''Extracts keywords from the user's query for exact search.'''\n",
    "       \n",
    "    keyword_extraction_prompt = \"\"\"\n",
    "\n",
    "    # Advanced Keyword Extraction for Technical Queries\n",
    "\n",
    "    You are an advanced AI assistant specialized in extracting crucial keywords from technical queries. Your expertise lies in pinpointing precise product information, including names, unique identifiers, and version details.\n",
    "    Use the Input Query marked by the ****.\n",
    "\n",
    "    ## Key Elements to Extract:\n",
    "\n",
    "    - **Product Name:** The primary identifier or common name of a product (e.g., Experion PKS).\n",
    "    - **SKU/Part Number:** Unique alphanumeric codes used to distinguish products or parts (e.g., RDYNAMO-8074, 1-EVDYZ2U).\n",
    "    - **Version/Release Info:** Specific version numbers or release identifiers for products or documents (e.g., R2.2.1, Update4).\n",
    "\n",
    "    ## Extraction Guidelines:\n",
    "\n",
    "    - **Priority:** Focus on extracting the product name first, followed by any specific identifiers (SKU/Part Number) or version/release information.\n",
    "    - **Patterns:** Pay attention to alphanumeric strings and version formats, which may include numbers, letters, hyphens, and periods.\n",
    "    - **Ignore Non-essential Words:** Overlook filler words such as \"is,\" \"in,\" \"for,\" etc., unless they are part of a product name or identifier.\n",
    "    - **Special Cases:** Treat terms like \"C300\" or \"301C\" as product names when not accompanied by other identifiable product names.\n",
    "\n",
    "    ## Task Execution:\n",
    "\n",
    "    1. Carefully review each query to discern the primary focus.\n",
    "    2. Extract relevant keywords, focusing on product names, part numbers/SKUs, and version/release information.\n",
    "    3. Present the extracted keywords separated by commas. For queries that focus on a specific functionality or general inquiry without mentioning a product/version, use \"no keyword found\"\n",
    "\n",
    "    ## Enhanced Examples:\n",
    "\n",
    "    - **Input Query:** \"Is there a Tools and Controller Update4 for R520.2?\"\n",
    "    - **Expected Keywords:** \"Tools and Controller, Update4, R520.2\"\n",
    "    - **Input Query:** \"What product anomalies are being introduced in Honeywell Forge Alarm Management Reporting R2.2.1?\"\n",
    "    - **Expected Keywords:** \"Honeywell Forge Alarm Management Reporting, R2.2.1\"\n",
    "    - **Input Query:** \"C300 sync steps for CBM6 v2\"\n",
    "    - **Expected Keywords:** \"C300, CBM6 v2\"\n",
    "    - **Input Query:** \"what is scada status in QuickBuilder?\"\n",
    "    - **Expected output:** \"no keyword found\"\n",
    "    - **Input Query:** \"C300 sync steps\"\n",
    "    - **Expected Keywords:** \"C300\"\n",
    "    - **Input Query:** \"What is a ‘ExpSQLAgtSvc’ account from local to domain users?\"\n",
    "    - **Expected Keywords:** \"‘ExpSQLAgtSvc’\"\n",
    "\n",
    "    Input Query: ****{raw_query}****\n",
    "    Keywords:\n",
    "    \"\"\"\n",
    "\n",
    "    keywork_prompt = PromptTemplate(\n",
    "        template = keyword_extraction_prompt,\n",
    "        input_variables = [\"raw_query\"]\n",
    "    ) \n",
    "\n",
    "    keyword_response_chain = keywork_prompt | chat_model\n",
    "    keyword_reponse = keyword_response_chain.invoke({\"raw_query\": user_query}).content\n",
    "    \n",
    "    return keyword_reponse\n",
    "\n",
    "\n",
    "\n",
    "def respond_using_documents(query, retrieved_docs, keywords=None):\n",
    "    '''Generates a response using the retrieved documents and the user's query.'''\n",
    "\n",
    "    prompt_prefix = \"\"\"Respond to the user query marked by #### based on the sources encapsulated within ****. Make sure to pay attention to the keywords marked by >>>>.\n",
    "    - Your response should strictly utilize the information from the provided Sources in line with user query.\n",
    "    - Every piece of information or fact you use from a source must be immediately cited within the sentence itself, using its \"fileName\", DO NOT USE ANY OTHER NAME. Example: \"The sky is blue [info.txt].\"\n",
    "    - DO NOT STATE CITATIONS at the very end of all the facts.\n",
    "    - If a single fact is backed by multiple sources, integrate each citation within the same sentence: \"The sky is blue [info1.txt][info2.txt].\"\n",
    "    - After formulating your answer, double-check for any inconsistencies or omitted citations.\n",
    "\n",
    "    STRICTLY frame your response as a valid JSON using the following fields :\n",
    "    - \"response\": The comprehensive answer to the user query with inline citations after each fact. For multiple lines, give a \"\\n\" seperated answer.\n",
    "    - \"confidence\": Quantify your confidence in the provided response on a numeric scale.\n",
    "    - \"reason\": Briefly (in no more than 20 words) elucidate the rationale behind your answer.\n",
    "\n",
    "    User query: ####{user_query}####\n",
    "\n",
    "    Sources: ****{retr_docs}****\n",
    "\n",
    "    Keywords: >>>>{keywords}>>>>\n",
    "    \n",
    "    Response: \"\"\" \n",
    "\n",
    "    final_prompt = PromptTemplate(\n",
    "        template = prompt_prefix,\n",
    "        input_variables = [\"user_query\", \"retr_docs\", \"keywords\"]\n",
    "    ) \n",
    "\n",
    "    final_response_chain = final_prompt | chat_model\n",
    "    final_reponse = final_response_chain.invoke({\"user_query\": query, \"retr_docs\": retrieved_docs, \"keywords\": keywords}).content\n",
    "\n",
    "\n",
    "    # Clean and parse the JSON\n",
    "    try:\n",
    "        final_reponse = final_reponse.strip('```json').strip()\n",
    "        cleaned_response = final_reponse.strip('```').strip()\n",
    "        data = json.loads(cleaned_response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error decoding JSON:\", e)\n",
    "        print(\"Raw response:\", final_reponse)\n",
    "\n",
    "    # Extract the desired fields\n",
    "    response = data.get(\"response\", \"\")\n",
    "    confidence = data.get(\"confidence\", 0)\n",
    "    reason = data.get(\"reason\", \"\")\n",
    "\n",
    "    return response, confidence, reason\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**callSystem() function to manage the response generation pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callSystem(query):\n",
    "    '''Main function to handle the user query and generate a response.'''\n",
    "\n",
    "    # User inputted query\n",
    "    print(\"\\nUser Query: \")\n",
    "    print(query)\n",
    "\n",
    "    # Extract keywords from the user query for exact search\n",
    "    keywords = get_keywords_for_exact_search(query)\n",
    "    print(\"\\nExtracted Keywords: \")\n",
    "    print(keywords)\n",
    "\n",
    "    # Rewrite the query to be more specific and concise\n",
    "    rewritten_question = rewrite_query(query)   \n",
    "    print(\"\\nRewritten Query: \")\n",
    "    print(rewritten_question)\n",
    "\n",
    "\n",
    "    # Use the rewritten question to search the vector store for relevant documents \n",
    "    retr_docs = retrieve_chunks(faiss_index, rewritten_question)['file_content'].to_list()\n",
    "\n",
    "    # Use the rewritten question and retrieved documents to generate a response\n",
    "    response, confidence, reason = respond_using_documents(rewritten_question, retr_docs, keywords)\n",
    "\n",
    "    # Print the extracted values\n",
    "    print(\"\\n\\nResponse:\")\n",
    "    print(response)\n",
    "    print(\"\\nConfidence:\")\n",
    "    print(confidence)\n",
    "    print(\"\\nReason:\")\n",
    "    print(reason)\n",
    "\n",
    "    return response, confidence, reason\n",
    "\n",
    "\n",
    "# For Testing the system\n",
    "#query = \"Do you know of a method, which does not involve a total rebuild, for changing the ‘ExpSQLSvc’ and ‘ExpSQLAgtSvc’ accounts from local to domain users, please?\"\n",
    "#callSystem(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate answers for all queries in the csv and export to new csv named \"Output_System_Responses.csv\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file of test queries into a DataFrame. test_queries_csv_path is set in the first cell of the notebook.\n",
    "queries_df = pd.read_csv(test_queries_csv_path, encoding='latin1')\n",
    "\n",
    "# The number of queries to process, set as high as needed.\n",
    "query_limit = 5\n",
    "\n",
    "# Path to the output CSV file\n",
    "responses_csv_file_path = 'Output_System_Responses.csv'\n",
    "\n",
    "# Write to the CSV file row by row\n",
    "with open(responses_csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['Index', 'casenumber', 'User_Query', \"System_Response\", \"ai_answer\"])\n",
    "    writer.writeheader()  # Write the header row\n",
    "\n",
    "    for index, row in queries_df.iterrows():\n",
    "        if index < query_limit:\n",
    "            print(\"\\nQuery: \" + str(index) + \"  -----------------------------------------\")\n",
    "            response = callSystem(row['User_Query'])\n",
    "            writer.writerow({\"Index\": index, \"casenumber\": row[\"casenumber\"], \"User_Query\": row['User_Query'], \"System_Response\": response[0], \"ai_answer\": row[\"ai_answer\"]})  # Write each row\n",
    "\n",
    "print(f\"Data has been written to {responses_csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load \"Output_System_Responses.csv\" and use it to evaluate the responses using RAGAs. Export results to \"Output_RAGAs.csv\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of responses to evaluate\n",
    "evaluation_limit = 1000\n",
    "\n",
    "evaluation_model = ChatDatabricks(\n",
    "    endpoint=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(responses_csv_file_path, encoding='latin1')\n",
    "\n",
    "# Path to the output CSV file\n",
    "output_csv_file_path = 'Output_RAGAs.csv'\n",
    "\n",
    "# Write to the CSV file row by row\n",
    "with open(output_csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['Index', 'casenumber', 'User_Query', \"System_Response\", \"Original_AI_Answer\",\n",
    "                                              \"Faithfulness\", \"Answer_Relevancy\", \"Context_Recall\", \"Context_Precision\"])\n",
    "    writer.writeheader()  # Write the header row\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if index < evaluation_limit:\n",
    "\n",
    "            # Embed the query\n",
    "            query_vector_embedding = get_embeddings(embedding_model, row['User_Query'])\n",
    "            query_vector = np.array(query_vector_embedding).astype(\"float32\").reshape(1,-1)\n",
    "            \n",
    "            # Use vector of query to search faiss for relevant docs\n",
    "            k=5\n",
    "            distance, indices = faiss_index.search(query_vector, k)\n",
    "            retrieved_chunks = metadata_df.iloc[indices[0]].reset_index(drop=True)\n",
    "\n",
    "\n",
    "            # Arrange information into a dictionary\n",
    "            data = {\n",
    "                \"question\": [row['User_Query']],\n",
    "                \"answer\": [row['System_Response']],\n",
    "                \"contexts\": [retrieved_chunks['file_content'].tolist()],\n",
    "                \"ground_truth\": [row[\"ai_answer\"]]\n",
    "            }\n",
    "\n",
    "            print(f\"\\nIndex: {index}-----------------------------------------\")\n",
    "            print(\"\\nQuestion:\")\n",
    "            print(data[\"question\"][0])\n",
    "            print(\"\\nResponse:\")\n",
    "            print(data[\"answer\"][0])\n",
    "            print(\"\\nGround_Truth:\")\n",
    "            print(data[\"ground_truth\"][0])\n",
    "            \n",
    "            # Convert dictionary into dataset\n",
    "            dataset = Dataset.from_dict(data)\n",
    "\n",
    "            # Evaluate with RAGAS\n",
    "            result = evaluate(\n",
    "                dataset = dataset,\n",
    "                metrics= [faithfulness, answer_relevancy, context_recall, context_precision],\n",
    "                llm = evaluation_model\n",
    "            )\n",
    "\n",
    "            print(\"RAGAS Score:\", result)\n",
    "\n",
    "            writer.writerow({\"Index\": index, \n",
    "                             \"casenumber\": row[\"casenumber\"], \n",
    "                             \"User_Query\": row['User_Query'], \n",
    "                             \"System_Response\": row[\"System_Response\"], \n",
    "                             \"Original_AI_Answer\": row[\"ai_answer\"],\n",
    "                             \"Faithfulness\":  round(result[\"faithfulness\"][0], 2), \n",
    "                             \"Answer_Relevancy\": round(result[\"answer_relevancy\"][0], 2),\n",
    "                             \"Context_Recall\": round(result[\"context_recall\"][0], 2), \n",
    "                             \"Context_Precision\": round(result[\"context_precision\"][0], 2)})  # Write each row"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "techgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
